{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ca6741",
   "metadata": {},
   "source": [
    "CLASSIFICATORE LESIONI PIGMENTATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms, datasets\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3983c39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagnosis_1\n",
      "Malignant    2156\n",
      "Benign       2156\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Chiede all'utente di inserire il percorso del file CSV da caricare\n",
    "csv_path = input(\"Inserisci il percorso del file CSV: \")\n",
    "\n",
    "# Legge il file CSV e lo salva in un DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Filtra il dataset tenendo solo le righe in cui la diagnosi è 'Benign' o 'Malignant'\n",
    "# (esclude quindi i casi con diagnosi 'Indifferent')\n",
    "df_filtrato = df[df['diagnosis_1'].isin(['Benign', 'Malignant'])]\n",
    "\n",
    "# Conta quante righe hanno la diagnosi 'Malignant'\n",
    "n_malignant = df[df['diagnosis_1'] == 'Malignant'].shape[0]\n",
    "\n",
    "# Campiona lo stesso numero di righe 'Benign'\n",
    "benign_sample = df[df['diagnosis_1'] == 'Benign'].sample(n=n_malignant, random_state=42)\n",
    "\n",
    "# Prende tutte le righe 'Malignant'\n",
    "malignant_all = df[df['diagnosis_1'] == 'Malignant']\n",
    "\n",
    "# Unisce i due insiemi (benigni campionati + tutti i maligni)\n",
    "df_balanced = pd.concat([benign_sample, malignant_all]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Controlla quante righe di 'Benign' e 'Malignant' ci sono nel dataset bilanciato\n",
    "print(df_balanced['diagnosis_1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7558c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chiede all'utente di inserire il percorso del file ZIP da caricare\n",
    "zip_path = input(\"Inserisci il percorso della ZIP: \")\n",
    "\n",
    "# Cartella di estrazione\n",
    "extract_folder = \"images\"\n",
    "\n",
    "# Cancella la cartella se esiste già\n",
    "if os.path.exists(extract_folder):\n",
    "    shutil.rmtree(extract_folder)\n",
    "\n",
    "# Crea la cartella di estrazione\n",
    "os.makedirs(extract_folder)\n",
    "\n",
    "# Estrae lo ZIP\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "# Rimuove la colonna 'image_path' da df_balanced se esiste\n",
    "if 'image_path' in df_balanced.columns:\n",
    "    df_balanced = df_balanced.drop(columns=['image_path'])\n",
    "\n",
    "# Crea la colonna 'image_path'\n",
    "df_balanced['image_path'] = df_balanced['isic_id'].apply(lambda x: os.path.join(extract_folder, f\"{x}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ea18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "Benign: 1509 immagini\n",
      "Malignant: 1509 immagini\n",
      "VALIDATION:\n",
      "Benign: 323 immagini\n",
      "Malignant: 323 immagini\n",
      "TEST:\n",
      "Benign: 324 immagini\n",
      "Malignant: 324 immagini\n"
     ]
    }
   ],
   "source": [
    "# Parametri\n",
    "base_dir = \"dataset_prepared\"\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "splits = ['train', 'validation', 'test']\n",
    "classes = ['Benign', 'Malignant']\n",
    "\n",
    "# Pulizia cartella\n",
    "if os.path.exists(base_dir):\n",
    "    shutil.rmtree(base_dir)\n",
    "\n",
    "# Crea cartelle\n",
    "for split in splits:\n",
    "    for cls in classes:\n",
    "        os.makedirs(os.path.join(base_dir, split, cls), exist_ok=True)\n",
    "\n",
    "# Divisione 70/15/15 da df_balanced\n",
    "for cls in classes:\n",
    "    subset = shuffle(df_balanced[df_balanced['diagnosis_1'] == cls], random_state=42)  # prende solo le immagini della classe corrente e le mescola casualmente\n",
    "\n",
    "    n_total = len(subset)\n",
    "    n_train = int(n_total * train_ratio)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    n_test = n_total - n_train - n_val  # assicura che la somma dia n_total\n",
    "    \n",
    "    train_df = subset.iloc[:n_train]  # prime n_train righe → train\n",
    "    val_df = subset.iloc[n_train:n_train + n_val] \n",
    "    test_df = subset.iloc[n_train + n_val:]\n",
    "\n",
    "    # Copia immagini per TRAIN\n",
    "    for _, row in train_df.iterrows():\n",
    "        src = row['image_path']  # percorso dell'immagine originale\n",
    "        dst = os.path.join(base_dir, 'train', cls, os.path.basename(src))  # percorso di destinazione\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "    # Copia immagini per VALIDATION\n",
    "    for _, row in val_df.iterrows():\n",
    "        src = row['image_path']\n",
    "        dst = os.path.join(base_dir, 'validation', cls, os.path.basename(src))\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "    # Copia immagini per TEST\n",
    "    for _, row in test_df.iterrows():\n",
    "        src = row['image_path']\n",
    "        dst = os.path.join(base_dir, 'test', cls, os.path.basename(src))\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# Stampa quante immagini sono contenute in ogni classe di ogni split\n",
    "for split in splits:\n",
    "    print(f\"{split.upper()}:\")\n",
    "    for cls in classes:\n",
    "        path = os.path.join(base_dir, split, cls)\n",
    "        print(f\"{cls}: {len(os.listdir(path))} immagini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e771ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device selezionato: cpu\n",
      "TRAIN:\n",
      "Epoch 1/50 | Train loss: 0.5216 | Train acc: 0.739 |New threshold: 0.462 | Sens: 0.907 | Spec: 0.700 | BalAcc: 0.803\n",
      "Epoch 2/50 | Train loss: 0.4234 | Train acc: 0.796 |New threshold: 0.390 | Sens: 0.904 | Spec: 0.687 | BalAcc: 0.796\n",
      "Epoch 3/50 | Train loss: 0.3768 | Train acc: 0.820 |New threshold: 0.467 | Sens: 0.901 | Spec: 0.737 | BalAcc: 0.819\n",
      "Epoch 4/50 | Train loss: 0.3360 | Train acc: 0.849 |New threshold: 0.278 | Sens: 0.904 | Spec: 0.743 | BalAcc: 0.824\n",
      "Epoch 5/50 | Train loss: 0.3083 | Train acc: 0.843 |New threshold: 0.406 | Sens: 0.910 | Spec: 0.737 | BalAcc: 0.824\n",
      "Epoch 6/50 | Train loss: 0.2641 | Train acc: 0.888 |New threshold: 0.431 | Sens: 0.904 | Spec: 0.746 | BalAcc: 0.825\n",
      "Epoch 7/50 | Train loss: 0.2257 | Train acc: 0.907 |New threshold: 0.444 | Sens: 0.901 | Spec: 0.780 | BalAcc: 0.841\n",
      "Epoch 8/50 | Train loss: 0.2149 | Train acc: 0.917 |New threshold: 0.439 | Sens: 0.901 | Spec: 0.780 | BalAcc: 0.841\n",
      "Epoch 9/50 | Train loss: 0.2063 | Train acc: 0.916 |New threshold: 0.407 | Sens: 0.901 | Spec: 0.793 | BalAcc: 0.847\n",
      "Epoch 10/50 | Train loss: 0.3027 | Train acc: 0.855 |New threshold: 0.359 | Sens: 0.901 | Spec: 0.771 | BalAcc: 0.836\n",
      "Epoch 11/50 | Train loss: 0.3008 | Train acc: 0.864 |New threshold: 0.528 | Sens: 0.901 | Spec: 0.740 | BalAcc: 0.820\n",
      "Epoch 12/50 | Train loss: 0.2668 | Train acc: 0.884 |New threshold: 0.518 | Sens: 0.904 | Spec: 0.802 | BalAcc: 0.853\n",
      "Epoch 13/50 | Train loss: 0.2279 | Train acc: 0.907 |New threshold: 0.245 | Sens: 0.901 | Spec: 0.774 | BalAcc: 0.837\n",
      "Epoch 14/50 | Train loss: 0.2131 | Train acc: 0.896 |New threshold: 0.429 | Sens: 0.901 | Spec: 0.737 | BalAcc: 0.819\n",
      "Epoch 15/50 | Train loss: 0.1911 | Train acc: 0.921 |New threshold: 0.482 | Sens: 0.907 | Spec: 0.771 | BalAcc: 0.839\n",
      "Epoch 16/50 | Train loss: 0.1538 | Train acc: 0.931 |New threshold: 0.354 | Sens: 0.904 | Spec: 0.771 | BalAcc: 0.837\n",
      "Epoch 17/50 | Train loss: 0.1560 | Train acc: 0.937 |New threshold: 0.177 | Sens: 0.910 | Spec: 0.768 | BalAcc: 0.839\n",
      "Epoch 18/50 | Train loss: 0.1195 | Train acc: 0.930 |New threshold: 0.269 | Sens: 0.901 | Spec: 0.765 | BalAcc: 0.833\n",
      "Epoch 19/50 | Train loss: 0.0855 | Train acc: 0.957 |New threshold: 0.107 | Sens: 0.907 | Spec: 0.740 | BalAcc: 0.824\n",
      "Epoch 20/50 | Train loss: 0.0802 | Train acc: 0.937 |New threshold: 0.316 | Sens: 0.901 | Spec: 0.777 | BalAcc: 0.839\n",
      "Epoch 21/50 | Train loss: 0.0655 | Train acc: 0.973 |New threshold: 0.209 | Sens: 0.901 | Spec: 0.805 | BalAcc: 0.853\n",
      "Epoch 22/50 | Train loss: 0.0514 | Train acc: 0.976 |New threshold: 0.321 | Sens: 0.901 | Spec: 0.774 | BalAcc: 0.837\n",
      "Epoch 23/50 | Train loss: 0.0521 | Train acc: 0.982 |New threshold: 0.188 | Sens: 0.907 | Spec: 0.737 | BalAcc: 0.822\n",
      "Epoch 24/50 | Train loss: 0.0404 | Train acc: 0.974 |New threshold: 0.290 | Sens: 0.901 | Spec: 0.749 | BalAcc: 0.825\n",
      "Epoch 25/50 | Train loss: 0.0351 | Train acc: 0.986 |New threshold: 0.127 | Sens: 0.907 | Spec: 0.759 | BalAcc: 0.833\n",
      "Epoch 26/50 | Train loss: 0.0355 | Train acc: 0.976 |New threshold: 0.252 | Sens: 0.901 | Spec: 0.768 | BalAcc: 0.834\n",
      "Epoch 27/50 | Train loss: 0.0325 | Train acc: 0.986 |New threshold: 0.226 | Sens: 0.904 | Spec: 0.762 | BalAcc: 0.833\n",
      "Epoch 28/50 | Train loss: 0.0262 | Train acc: 0.990 |New threshold: 0.114 | Sens: 0.907 | Spec: 0.755 | BalAcc: 0.831\n",
      "Epoch 29/50 | Train loss: 0.0250 | Train acc: 0.981 |New threshold: 0.197 | Sens: 0.904 | Spec: 0.762 | BalAcc: 0.833\n",
      "Epoch 30/50 | Train loss: 0.0754 | Train acc: 0.962 |New threshold: 0.106 | Sens: 0.907 | Spec: 0.759 | BalAcc: 0.833\n",
      "Epoch 31/50 | Train loss: 0.1451 | Train acc: 0.912 |New threshold: 0.319 | Sens: 0.901 | Spec: 0.737 | BalAcc: 0.819\n",
      "Epoch 32/50 | Train loss: 0.0994 | Train acc: 0.957 |New threshold: 0.652 | Sens: 0.910 | Spec: 0.802 | BalAcc: 0.856\n",
      "Epoch 33/50 | Train loss: 0.1147 | Train acc: 0.950 |New threshold: 0.131 | Sens: 0.901 | Spec: 0.768 | BalAcc: 0.834\n",
      "Epoch 34/50 | Train loss: 0.1147 | Train acc: 0.931 |New threshold: 0.671 | Sens: 0.904 | Spec: 0.715 | BalAcc: 0.810\n",
      "Epoch 35/50 | Train loss: 0.0731 | Train acc: 0.968 |New threshold: 0.164 | Sens: 0.901 | Spec: 0.731 | BalAcc: 0.816\n",
      "Epoch 36/50 | Train loss: 0.0665 | Train acc: 0.962 |New threshold: 0.028 | Sens: 0.923 | Spec: 0.755 | BalAcc: 0.839\n",
      "Epoch 37/50 | Train loss: 0.0779 | Train acc: 0.920 |New threshold: 0.085 | Sens: 0.901 | Spec: 0.796 | BalAcc: 0.848\n",
      "Epoch 38/50 | Train loss: 0.0531 | Train acc: 0.961 |New threshold: 0.056 | Sens: 0.904 | Spec: 0.709 | BalAcc: 0.807\n",
      "Epoch 39/50 | Train loss: 0.0654 | Train acc: 0.945 |New threshold: 0.162 | Sens: 0.901 | Spec: 0.759 | BalAcc: 0.830\n",
      "Epoch 40/50 | Train loss: 0.0584 | Train acc: 0.967 |New threshold: 0.257 | Sens: 0.907 | Spec: 0.783 | BalAcc: 0.845\n",
      "Epoch 41/50 | Train loss: 0.0553 | Train acc: 0.977 |New threshold: 0.436 | Sens: 0.901 | Spec: 0.777 | BalAcc: 0.839\n",
      "Epoch 42/50 | Train loss: 0.0358 | Train acc: 0.988 |New threshold: 0.138 | Sens: 0.901 | Spec: 0.777 | BalAcc: 0.839\n",
      "Epoch 43/50 | Train loss: 0.0405 | Train acc: 0.980 |New threshold: 0.429 | Sens: 0.901 | Spec: 0.786 | BalAcc: 0.844\n",
      "Epoch 44/50 | Train loss: 0.0448 | Train acc: 0.982 |New threshold: 0.193 | Sens: 0.901 | Spec: 0.802 | BalAcc: 0.851\n",
      "Epoch 45/50 | Train loss: 0.0369 | Train acc: 0.981 |New threshold: 0.217 | Sens: 0.901 | Spec: 0.765 | BalAcc: 0.833\n",
      "Epoch 46/50 | Train loss: 0.0304 | Train acc: 0.984 |New threshold: 0.259 | Sens: 0.901 | Spec: 0.774 | BalAcc: 0.837\n",
      "Epoch 47/50 | Train loss: 0.0181 | Train acc: 0.993 |New threshold: 0.238 | Sens: 0.904 | Spec: 0.774 | BalAcc: 0.839\n",
      "Epoch 48/50 | Train loss: 0.0146 | Train acc: 0.994 |New threshold: 0.186 | Sens: 0.904 | Spec: 0.765 | BalAcc: 0.834\n",
      "Epoch 49/50 | Train loss: 0.0128 | Train acc: 0.992 |New threshold: 0.202 | Sens: 0.901 | Spec: 0.799 | BalAcc: 0.850\n",
      "Epoch 50/50 | Train loss: 0.0126 | Train acc: 0.992 |New threshold: 0.110 | Sens: 0.904 | Spec: 0.765 | BalAcc: 0.834\n",
      "Accuratezza migliore ottenuto all'epoca: 32\n",
      "TEST:\n",
      "Risultati Test:\n",
      "Benigni corretti (TN): 257 (39.7%)\n",
      "Maligni corretti (TP): 300 (46.3%)\n",
      "Benigni sbagliati (FP): 67 (10.3%)\n",
      "Maligni sbagliati (FN): 24 (3.7%)\n",
      "Sensibilità: 0.926 | Specificità: 0.793 | Accuratezza bilanciata: 0.860\n"
     ]
    }
   ],
   "source": [
    "# Selezione del dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device selezionato:\", device)\n",
    "\n",
    "\n",
    "# Configurazione generale\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "img_size = 224\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "best_threshold = 0.5  # default iniziale\n",
    "num_workers = 2\n",
    "checkpoint_path = \"best_model.pth\"  #percorso dove salverà il modello migliore\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "# Trasformazioni delle immagini\n",
    "\n",
    "# Trasformazioni per il training\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Trasformazioni per validation/test\n",
    "val_test_tf = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Dataset\n",
    "train_ds = datasets.ImageFolder(os.path.join(base_dir, \"train\"), transform=train_tf)\n",
    "val_ds   = datasets.ImageFolder(os.path.join(base_dir, \"validation\"), transform=val_test_tf)\n",
    "test_ds  = datasets.ImageFolder(os.path.join(base_dir, \"test\"), transform=val_test_tf)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=num_workers, pin_memory=torch.cuda.is_available())\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=num_workers, pin_memory=torch.cuda.is_available())\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n",
    "                         num_workers=num_workers, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "# Modello RESNET50\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # Sostituisco l'ultimo layer per adattare il modello a 2 classi\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Learning rate differenziati\n",
    "\n",
    "# ResNet50 è composta da:\n",
    "# -layer iniziale\n",
    "# -layer 1: 3 blocchi\n",
    "# -layer 2: 4 blocchi\n",
    "# -layer 3: 6 blocchi\n",
    "# -layer 4: 3 blocchi\n",
    "# -layer finale\n",
    "\n",
    "# Prende tutti i blocchi residui della RESNET50\n",
    "def count_residual_units_resnet50(model):\n",
    "    units = []\n",
    "    for lname in ['layer1','layer2','layer3','layer4']:\n",
    "        for blk in getattr(model, lname):\n",
    "            units.append(blk)\n",
    "    return units\n",
    "\n",
    "# Divide i parametri del modello in gruppi;\n",
    "# asssegna a ciascun gruppo un learning rate differente\n",
    "def make_param_groups(model):\n",
    "    units = count_residual_units_resnet50(model)\n",
    "    first6, next8, rest = [], [], []\n",
    "    # Aggiunge i parametri dei primi 6 blocchi a first6\n",
    "    for blk in units[:6]:\n",
    "        first6 += list(blk.parameters())\n",
    "    # Aggiunge i parametri dei successivi 10 blocchi a next8\n",
    "    for blk in units[6:16]:\n",
    "        next8 += list(blk.parameters())\n",
    "    # Aggiunge i parametri del layer finale a fc_params\n",
    "    fc_params = list(model.fc.parameters())\n",
    "    # Inserisce eventuali parametri riamnenti in rest\n",
    "    covered = set([id(p) for p in first6 + next8 + fc_params])\n",
    "    for p in model.parameters():\n",
    "        if id(p) not in covered:\n",
    "            rest.append(p)\n",
    "    return [\n",
    "        {\"params\": first6, \"lr\": 0.009},\n",
    "        {\"params\": next8 + rest, \"lr\": 0.003},\n",
    "        {\"params\": fc_params, \"lr\": 0.01}\n",
    "    ]\n",
    "\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#1. Ottimizza grazie a  SGD con momentum e weight decay\n",
    "optimizer = optim.SGD(make_param_groups(model), momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "#2. Aggiorna il learning rate secondo una funzione cosine annealing con warm restart\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "#1+2. SGDR\n",
    "\n",
    "\n",
    "# Cerca la soglia che garantisce almeno la sensibilità target e minimizza i falsi positivi\n",
    "def find_best_threshold(model, loader, device, target_sens=0.9):\n",
    "\n",
    "    model.eval()  # Mette il modello in modalità test\n",
    "\n",
    "    y_true, y_score = [], []  # y_true saranno le etichette, y_score le probabilità predette\n",
    "    with torch.no_grad():  # non calcola i gradienti\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            logits = model(imgs)  # logits, tensor (batch_size, 2), rappresenta i valori grezzi prodotti dall’ultimo layer del modello, prima di essere trasformati in probabilità\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1]  # probabilità della classe Malignant\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_score.extend(probs.cpu().numpy())\n",
    "    y_true = np.array(y_true)\n",
    "    y_score = np.array(y_score)\n",
    "\n",
    "    if len(np.unique(y_true)) < 2:  # se le etichette sono tutti 0 o tutti 1\n",
    "        return 0.5\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)  # vettore della percentuale di falsi positivi per ogni soglia, vettore della sensibilità per ogni soglia, vettore delle soglie\n",
    "    idx_ok = np.where(tpr >= target_sens)[0]   # indice delle soglie che garantiscono la sensibilità minima\n",
    "    if len(idx_ok) == 0:\n",
    "        # Se non raggiunge la sensibilità desiderata, scegle la soglia con massimo tpr\n",
    "        best_idx = np.argmax(tpr)\n",
    "        return float(thresholds[best_idx])\n",
    "    # Tra le soglie ok, scegle quella con minimo FPR\n",
    "    best_idx = idx_ok[np.argmin(fpr[idx_ok])]\n",
    "    return float(thresholds[best_idx])\n",
    "\n",
    "\n",
    "def predict_with_threshold(logits, threshold=0.5):\n",
    "    probs = torch.softmax(logits, dim=1)[:, 1] \n",
    "    return (probs >= threshold).long()  # ritorna 1 se maligno, 0 se benigno\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()  # Mette il modello in modalità train\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()  # azzera gradienti\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()  # backpropagation\n",
    "        optimizer.step()  # aggiorna i pesi\n",
    "        scheduler.step(epoch + batch_idx / max(1, len(train_loader)))  # aggiorna il learning rate in base al punto preciso dell’allenamento\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        preds = predict_with_threshold(logits, threshold=best_threshold)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += imgs.size(0)\n",
    "    avg_loss = total_loss / total if total > 0 else 0.0\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "# Calcola sensibilità, specificità e bilanciamento\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    bal_acc = 0.5 * (sens + spec)\n",
    "    return sens, spec, bal_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def results(model, loader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        logits = model(imgs)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        preds = predict_with_threshold(logits, threshold)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    except ValueError:  #\n",
    "        tn = fp = fn = tp = 0\n",
    "        for yt, yp in zip(y_true, y_pred):\n",
    "            if yt == 0 and yp == 0: tn += 1\n",
    "            elif yt == 0 and yp == 1: fp += 1\n",
    "            elif yt == 1 and yp == 0: fn += 1\n",
    "            elif yt == 1 and yp == 1: tp += 1\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    bal_acc = 0.5 * (sens + spec)\n",
    "    return sens, spec, bal_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def print_results(model, loader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        logits = model(imgs)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        preds = predict_with_threshold(logits, threshold)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    except ValueError:  #\n",
    "        tn = fp = fn = tp = 0\n",
    "        for yt, yp in zip(y_true, y_pred):\n",
    "            if yt == 0 and yp == 0: tn += 1\n",
    "            elif yt == 0 and yp == 1: fp += 1\n",
    "            elif yt == 1 and yp == 0: fn += 1\n",
    "            elif yt == 1 and yp == 1: tp += 1\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    bal_acc = 0.5 * (sens + spec)\n",
    "    print(\"Risultati Test:\")\n",
    "    print(f\"Benigni corretti (TN): {tn} ({(tn/len(y_true)*100) if len(y_true)>0 else 0:.1f}%)\")\n",
    "    print(f\"Maligni corretti (TP): {tp} ({(tp/len(y_true)*100) if len(y_true)>0 else 0:.1f}%)\")\n",
    "    print(f\"Benigni sbagliati (FP): {fp} ({(fp/len(y_true)*100) if len(y_true)>0 else 0:.1f}%)\")\n",
    "    print(f\"Maligni sbagliati (FN): {fn} ({(fn/len(y_true)*100) if len(y_true)>0 else 0:.1f}%)\")\n",
    "    print(f\"Sensibilità: {sens:.3f} | Specificità: {spec:.3f} | Accuratezza bilanciata: {bal_acc:.3f}\")\n",
    "    return sens, spec, bal_acc\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "#TRAIN\n",
    "print(\"TRAIN:\")\n",
    "\n",
    "best_bal_acc = -1.0 \n",
    "best_epoch = -1\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    tr_loss, tr_acc = train_one_epoch(epoch)\n",
    "    best_threshold = find_best_threshold(model, val_loader, device, target_sens=0.9)\n",
    "    sens, spec, bal_acc = results(model, val_loader, device, threshold=best_threshold)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} | Train loss: {tr_loss:.4f} | Train acc: {tr_acc:.3f} |\"\n",
    "          f\"New threshold: {best_threshold:.3f} | Sens: {sens:.3f} | Spec: {spec:.3f} | BalAcc: {bal_acc:.3f}\")\n",
    "\n",
    "    #Salva il modello migliore\n",
    "    if bal_acc > best_bal_acc:\n",
    "        best_bal_acc = bal_acc\n",
    "        best_epoch = epoch\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"best_threshold\": best_threshold,\n",
    "            \"best_bal_acc\": best_bal_acc\n",
    "        }, checkpoint_path)\n",
    "\n",
    "print(f\"Accuratezza migliore ottenuto all'epoca: {best_epoch}\")\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "#TEST\n",
    "print(\"TEST:\")\n",
    "\n",
    "# Usa il modello migliore sul test set\n",
    "if os.path.exists(checkpoint_path):\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    best_threshold = ckpt.get(\"best_threshold\", best_threshold)\n",
    "print_results(model, test_loader, device, threshold=best_threshold);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
